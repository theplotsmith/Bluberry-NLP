{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b1787a",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3b30e",
   "metadata": {},
   "source": [
    "## load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400b4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers \n",
    "#!pip install textblob \n",
    "#!pip install pyvis\n",
    "#!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d8bc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "## Load Packages\n",
    "import torch\n",
    "from transformers import pipeline \n",
    "summarizer = pipeline (\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee6f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workmen \n",
    "# Import numpy and pandas to work with dataframes \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import networkx as nx \n",
    "from pyvis.network import Network\n",
    "\n",
    "# Import seaborn and matplotlib for viz \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import stopwords \n",
    "#nltk.download('punkt')\n",
    "#nltk.download ('averaged_perceptron_tagger') \n",
    "#nltk.download('brown')\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk import punkt\n",
    "\n",
    "# download stop words\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('popular')\n",
    "stop_words = stopwords.words('english')\n",
    "custom_stopwords = ['', '']\n",
    "\n",
    "\n",
    "# Import textblob \n",
    "from textblob import Word, TextBlob\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a3e735",
   "metadata": {},
   "source": [
    "## Load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ea8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load text\n",
    "raw_data = pd.read_csv('G:\\\\My Drive\\\\01. PlotSmith\\\\06. Tinkering\\\\Python\\\\trustpilot text NLP\\\\blueberry_text.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99197f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24/10/2023</td>\n",
       "      <td>sample 1</td>\n",
       "      <td>Sure! Overall, my experience with Blueberry ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24/10/2023</td>\n",
       "      <td>sample 2</td>\n",
       "      <td>Well, to be honest, my overall experience with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     title                                             detail\n",
       "0  24/10/2023  sample 1  Sure! Overall, my experience with Blueberry ha...\n",
       "1  24/10/2023  sample 2  Well, to be honest, my overall experience with..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db1171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean questionable characters \n",
    "raw_data = raw_data.replace(\",\", regex=True)\n",
    "raw_data = raw_data.replace(\"!\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a69bd",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d103c457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\peter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\peter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet') \n",
    "stop_words = stopwords.words ('english') \n",
    "custom_stopwords = [\", \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f36684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>detail</th>\n",
       "      <th>Processed rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24/10/2023</td>\n",
       "      <td>sample 1</td>\n",
       "      <td>Sure! Overall, my experience with Blueberry ha...</td>\n",
       "      <td>sure! overall, experience blueberry quite posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24/10/2023</td>\n",
       "      <td>sample 2</td>\n",
       "      <td>Well, to be honest, my overall experience with...</td>\n",
       "      <td>well, honest, overall experience blueberry qui...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     title                                             detail  \\\n",
       "0  24/10/2023  sample 1  Sure! Overall, my experience with Blueberry ha...   \n",
       "1  24/10/2023  sample 2  Well, to be honest, my overall experience with...   \n",
       "\n",
       "                                       Processed rev  \n",
       "0  sure! overall, experience blueberry quite posi...  \n",
       "1  well, honest, overall experience blueberry qui...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_revs(rev, custom_stopwords):\n",
    "    processed_rev = rev \n",
    "    processed_rev.replace('[^\\w\\s]', '') \n",
    "    processed_rev = \" \".join(word for word in processed_rev.split() if word not in stop_words) \n",
    "    processed_rev = \" \".join(word for word in processed_rev.split() if word not in custom_stopwords) \n",
    "    processed_rev = \" \".join(Word(word). lemmatize() for word in processed_rev.split()) \n",
    "    return(processed_rev)\n",
    "                          \n",
    "raw_data[ 'Processed rev'] = raw_data['detail'].apply(lambda x: preprocess_revs(x, custom_stopwords)) \n",
    "raw_data['Processed rev'] = raw_data['Processed rev'].apply(lambda x: x.lower()) \n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98506f",
   "metadata": {},
   "source": [
    "## Summarize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a3364c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['summary'] = raw_data['detail'].apply(lambda x: summarizer(x, max_length=50, min_length=15, do_sample=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d26075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(raw_data)):\n",
    "    raw_data['summary'][i] =     raw_data['summary'][i][0]['summary_text']\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65d168",
   "metadata": {},
   "source": [
    "# Product mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "504058fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "blueberry_handle = ['blueberry', 'blueberry'] \n",
    "shipping_handle = ['shipping', 'shipping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b44b495f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>detail</th>\n",
       "      <th>Processed rev</th>\n",
       "      <th>summary</th>\n",
       "      <th>blueberry</th>\n",
       "      <th>shipping</th>\n",
       "      <th>nether</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24/10/2023</td>\n",
       "      <td>sample 1</td>\n",
       "      <td>Sure! Overall, my experience with Blueberry ha...</td>\n",
       "      <td>sure! overall, experience blueberry quite posi...</td>\n",
       "      <td>Blueberry offers a wide range of products, co...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24/10/2023</td>\n",
       "      <td>sample 2</td>\n",
       "      <td>Well, to be honest, my overall experience with...</td>\n",
       "      <td>well, honest, overall experience blueberry qui...</td>\n",
       "      <td>Blueberry's customer reviews seemed unreliabl...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     title                                             detail  \\\n",
       "0  24/10/2023  sample 1  Sure! Overall, my experience with Blueberry ha...   \n",
       "1  24/10/2023  sample 2  Well, to be honest, my overall experience with...   \n",
       "\n",
       "                                       Processed rev  \\\n",
       "0  sure! overall, experience blueberry quite posi...   \n",
       "1  well, honest, overall experience blueberry qui...   \n",
       "\n",
       "                                             summary  blueberry  shipping  \\\n",
       "0   Blueberry offers a wide range of products, co...          1         1   \n",
       "1   Blueberry's customer reviews seemed unreliabl...          1         0   \n",
       "\n",
       "   nether  \n",
       "0       0  \n",
       "1       0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def identify_subject(rev, refs):\n",
    "    flag = 0 \n",
    "    for ref in refs: \n",
    "        if rev.find(ref) != -1:\n",
    "            flag = 1 \n",
    "    return flag\n",
    "\n",
    "\n",
    "raw_data['blueberry'] = raw_data['Processed rev'].apply(lambda x: identify_subject(x, blueberry_handle)) \n",
    "raw_data['shipping'] = raw_data['Processed rev'].apply(lambda x: identify_subject(x, shipping_handle))\n",
    "\n",
    "def notopic(blueberry, shipping): \n",
    "    if blueberry==1 or shipping==1:\n",
    "        nither=0 \n",
    "    else:\n",
    "        nither=1 \n",
    "    return nither\n",
    "\n",
    "                               \n",
    "raw_data['nether'] = raw_data.apply(lambda x: notopic(x['blueberry'], x['shipping']), axis=1) \n",
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b0c8f4",
   "metadata": {},
   "source": [
    "## Sentiment and polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3f2d32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed rev</th>\n",
       "      <th>blueberry</th>\n",
       "      <th>shipping</th>\n",
       "      <th>nether</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sure! overall, experience blueberry quite posi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236185</td>\n",
       "      <td>0.488811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>well, honest, overall experience blueberry qui...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.048086</td>\n",
       "      <td>0.504279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Processed rev  blueberry  shipping  \\\n",
       "0  sure! overall, experience blueberry quite posi...          1         1   \n",
       "1  well, honest, overall experience blueberry qui...          1         0   \n",
       "\n",
       "   nether  polarity  subjectivity  \n",
       "0       0  0.236185      0.488811  \n",
       "1       0 -0.048086      0.504279  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate polarity \n",
    "raw_data[ 'polarity'] = raw_data[ 'Processed rev'].apply(lambda x: TextBlob(x). sentiment[0]) \n",
    "raw_data['subjectivity'] = raw_data['Processed rev'].apply(lambda x: TextBlob(x). sentiment[1]) \n",
    "raw_data[['Processed rev', 'blueberry', 'shipping', 'nether', 'polarity', 'subjectivity']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "376ba740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">polarity</th>\n",
       "      <th colspan=\"4\" halign=\"left\">subjectivity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blueberry</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.09405</td>\n",
       "      <td>0.236185</td>\n",
       "      <td>-0.048086</td>\n",
       "      <td>0.09405</td>\n",
       "      <td>0.496545</td>\n",
       "      <td>0.504279</td>\n",
       "      <td>0.488811</td>\n",
       "      <td>0.496545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          polarity                              subjectivity            \\\n",
       "              mean       max       min   median         mean       max   \n",
       "blueberry                                                                \n",
       "1          0.09405  0.236185 -0.048086  0.09405     0.496545  0.504279   \n",
       "\n",
       "                               \n",
       "                min    median  \n",
       "blueberry                      \n",
       "1          0.488811  0.496545  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">polarity</th>\n",
       "      <th colspan=\"4\" halign=\"left\">subjectivity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shipping</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.236185</td>\n",
       "      <td>0.236185</td>\n",
       "      <td>0.236185</td>\n",
       "      <td>0.236185</td>\n",
       "      <td>0.488811</td>\n",
       "      <td>0.488811</td>\n",
       "      <td>0.488811</td>\n",
       "      <td>0.488811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          polarity                               subjectivity            \\\n",
       "              mean       max       min    median         mean       max   \n",
       "shipping                                                                  \n",
       "1         0.236185  0.236185  0.236185  0.236185     0.488811  0.488811   \n",
       "\n",
       "                              \n",
       "               min    median  \n",
       "shipping                      \n",
       "1         0.488811  0.488811  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">polarity</th>\n",
       "      <th colspan=\"4\" halign=\"left\">subjectivity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nether</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [(polarity, mean), (polarity, max), (polarity, min), (polarity, median), (subjectivity, mean), (subjectivity, max), (subjectivity, min), (subjectivity, median)]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(raw_data[raw_data['blueberry' ]==1][['blueberry' , 'polarity', 'subjectivity']].groupby('blueberry' ).agg([np.mean, np.max, np.min, np.median])) \n",
    "display(raw_data[raw_data['shipping']==1][['shipping', 'polarity', 'subjectivity']].groupby('shipping').agg([np.mean, np.max, np.min, np.median])) \n",
    "display(raw_data[raw_data[ 'nether']==1][[ 'nether', 'polarity', 'subjectivity']].groupby('nether' ).agg([np.mean, np.max, np.min, np.median]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1929b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = ConllExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97fab0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mconll2000\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('conll2000')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/conll2000\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\peter/nltk_data'\n",
      "    - 'C:\\\\Users\\\\peter\\\\anaconda3\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\peter\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\peter\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\peter\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n"
     ]
    },
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mconll2000\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('conll2000')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/conll2000.zip/conll2000/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\peter/nltk_data'\n    - 'C:\\\\Users\\\\peter\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\peter\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\peter\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\peter\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\decorators.py:35\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\en\\np_extractors.py:23\u001b[0m, in \u001b[0;36mChunkParser.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m'''Train the Chunker on the ConLL-2000 corpus.'''\u001b[39;00m\n\u001b[0;32m     21\u001b[0m train_data \u001b[38;5;241m=\u001b[39m [[(t, c) \u001b[38;5;28;01mfor\u001b[39;00m _, t, c \u001b[38;5;129;01min\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mchunk\u001b[38;5;241m.\u001b[39mtree2conlltags(sent)]\n\u001b[0;32m     22\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m               \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconll2000\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunked_sents\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     24\u001b[0m                                             chunk_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNP\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[0;32m     25\u001b[0m unigram_tagger \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mUnigramTagger(train_data)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mconll2000\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('conll2000')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/conll2000\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\peter/nltk_data'\n    - 'C:\\\\Users\\\\peter\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\peter\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\peter\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\peter\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m raw_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m raw_data[ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed rev\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: TextBlob(x)\u001b[38;5;241m.\u001b[39mwords) \n\u001b[0;32m      2\u001b[0m raw_data[ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m raw_data[ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed rev\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: TextBlob(x)\u001b[38;5;241m.\u001b[39mtags)\n\u001b[1;32m----> 3\u001b[0m raw_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoun_phrase\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mraw_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProcessed rev\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextractor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoun_phrases\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m raw_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m raw_data[ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed rev\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: TextBlob(x)\u001b[38;5;241m.\u001b[39mwords) \n\u001b[0;32m      2\u001b[0m raw_data[ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m raw_data[ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed rev\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: TextBlob(x)\u001b[38;5;241m.\u001b[39mtags)\n\u001b[1;32m----> 3\u001b[0m raw_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoun_phrase\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m raw_data[ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed rev\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextractor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoun_phrases\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\decorators.py:24\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m---> 24\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\blob.py:483\u001b[0m, in \u001b[0;36mBaseBlob.noun_phrases\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnoun_phrases\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a list of noun phrases for this blob.\"\"\"\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m WordList([phrase\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m--> 483\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnp_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m                     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(phrase) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\en\\np_extractors.py:69\u001b[0m, in \u001b[0;36mConllExtractor.extract\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     67\u001b[0m noun_phrases \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m---> 69\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Get the string representation of each subtree that is a\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# noun phrase tree\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     phrases \u001b[38;5;241m=\u001b[39m [_normalize_tags(filter_insignificant(each,\n\u001b[0;32m     73\u001b[0m                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mINSIGNIFICANT_SUFFIXES)) \u001b[38;5;28;01mfor\u001b[39;00m each \u001b[38;5;129;01min\u001b[39;00m parsed\n\u001b[0;32m     74\u001b[0m                \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(each, nltk\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mTree) \u001b[38;5;129;01mand\u001b[39;00m each\u001b[38;5;241m.\u001b[39mlabel()\n\u001b[0;32m     75\u001b[0m                \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNP\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filter_insignificant(each)) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     76\u001b[0m                \u001b[38;5;129;01mand\u001b[39;00m _is_match(each, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCFG)]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\en\\np_extractors.py:84\u001b[0m, in \u001b[0;36mConllExtractor._parse_sentence\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m'''Tag and parse a sentence (a plain, untagged string).'''\u001b[39;00m\n\u001b[0;32m     83\u001b[0m tagged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPOS_TAGGER\u001b[38;5;241m.\u001b[39mtag(sentence)\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtagged\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\en\\np_extractors.py:32\u001b[0m, in \u001b[0;36mChunkParser.parse\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m'''Return the parse tree for the sentence.'''\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trained:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m [pos \u001b[38;5;28;01mfor\u001b[39;00m (word, pos) \u001b[38;5;129;01min\u001b[39;00m sentence]\n\u001b[0;32m     34\u001b[0m tagged_pos_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagger\u001b[38;5;241m.\u001b[39mtag(pos_tags)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\decorators.py:38\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(err)\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingCorpusError()\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "raw_data['words'] = raw_data[ 'Processed rev'].apply(lambda x: TextBlob(x).words) \n",
    "raw_data[ 'tags'] = raw_data[ 'Processed rev'].apply(lambda x: TextBlob(x).tags)\n",
    "raw_data['noun_phrase'] = raw_data[ 'Processed rev'].apply(lambda x: TextBlob(x, np_extractor=extractor).noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['index1'] = raw_data.index \n",
    "noun_phrase = raw_data[['index1', 'noun_phrase']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d008b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_phrase.explode('noun_phrase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4826e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_text =\n",
    "for i in range(0, len(raw_data['Processed rev'])):\n",
    "    raw_data[ 'Processed rev'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['bigrams'] = raw_data['Processed rev'].apply(lambda x: TextBlob(x).ngrams (n=2) )\n",
    "raw_data['bigrams'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9184845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'first get individual words'\n",
    "raw_data['tokens'] = raw_data['Processed rev'].apply(lambda x: TextBlob(x).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da069033",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [] \n",
    "for doc in raw_data['tokens'] :\n",
    "    bigrams.extend([(doc[i-1], doc[i])\n",
    "        for i in range(1, len(doc))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e510517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_freq = [(b,bigrams.count(b))\n",
    "               for b in set(bigrams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_freq_df = pd.DataFrame(bigrams_freq, columns = ['SourceTarget', 'Weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c10aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_freq_df['Source'] = bigrams_freq_df[ 'SourceTarget'].apply(lambda x: x[0]) \n",
    "bigrams_freq_df['Target'] = bigrams_freq_df[ 'SourceTarget'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_freq_df['Type'] = 'Undirected'\n",
    "bigrams_freq_df[['Source', 'Target', 'Type', 'Weight']]\n",
    "net_data = bigrams_freq_df[bigrams_freq_df['Weight'] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G = nx.from_pandas_edgelist(net_data, source = 'Source', target = 'Target', edge_attr='Weight')\n",
    "\n",
    "net = Network(notebook = True) \n",
    "net.from_nx(G) \n",
    "net.show(\"example.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551dea91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c6c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
